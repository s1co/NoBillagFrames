{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere Bibliotheken\n",
    "import os\n",
    "import re\n",
    "import urllib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Retweet</th>\n",
       "      <th>RT_from</th>\n",
       "      <th>Orig_Tweet</th>\n",
       "      <th>Tweet_Participants</th>\n",
       "      <th>Reply</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Lang</th>\n",
       "      <th>Link</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-18 17:07:00</th>\n",
       "      <td>@GeroldSchlegel</td>\n",
       "      <td>RT @redder66: @NatalieRickli @NoBillag Die Unt...</td>\n",
       "      <td>True</td>\n",
       "      <td>@redder66</td>\n",
       "      <td>False</td>\n",
       "      <td>[@NatalieRickli, @NoBillag, @SVPch]</td>\n",
       "      <td>False</td>\n",
       "      <td>Die Unterstützung der für die #NoBillag ergibt...</td>\n",
       "      <td>[NoBillag, NeinzuNoBillag]</td>\n",
       "      <td>de</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-23 13:53:00</th>\n",
       "      <td>@derlamentierer</td>\n",
       "      <td>@ProBillag auch sie haben das konzept von soli...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>[@ProBillag]</td>\n",
       "      <td>True</td>\n",
       "      <td>auch sie haben das konzept von solidarität nic...</td>\n",
       "      <td>[nobillag, jazunobillag, NeinzuNoBillag]</td>\n",
       "      <td>de</td>\n",
       "      <td>[https://t.co/iFLeHBuSFS]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-05 19:47:00</th>\n",
       "      <td>@FlorianSchwab</td>\n",
       "      <td>RT @VitalSteiner: Was die Gegner von #NoBillag...</td>\n",
       "      <td>True</td>\n",
       "      <td>@VitalSteiner</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>Was die Gegner von #NoBillag mit solchen Bilde...</td>\n",
       "      <td>[NoBillag]</td>\n",
       "      <td>de</td>\n",
       "      <td>[https://t.co/IKOSeftUlm]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-17 07:08:00</th>\n",
       "      <td>@Schweizer2018</td>\n",
       "      <td>RT @realDanielVozar: Wenn #NoBillag abgelehnt ...</td>\n",
       "      <td>True</td>\n",
       "      <td>@realDanielVozar</td>\n",
       "      <td>False</td>\n",
       "      <td>[@LHeimgartner]</td>\n",
       "      <td>False</td>\n",
       "      <td>Wenn #NoBillag abgelehnt wird, wird sich gar n...</td>\n",
       "      <td>[NoBillag, JazuNoBillag]</td>\n",
       "      <td>de</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19 09:52:00</th>\n",
       "      <td>@PilgramsWords</td>\n",
       "      <td>RT @_macmike: #StarkesStück https://t.co/2L7uG...</td>\n",
       "      <td>True</td>\n",
       "      <td>@_macmike</td>\n",
       "      <td>False</td>\n",
       "      <td>[@Blickch, @20min, @bazonline]</td>\n",
       "      <td>False</td>\n",
       "      <td>#StarkesStück  #AfD #SVP #ARD #ZDF #ORF #SRF #...</td>\n",
       "      <td>[StarkesStück, AfD, SVP, ARD, ZDF, ORF, SRF, s...</td>\n",
       "      <td>de</td>\n",
       "      <td>[https://t.co/2L7uGmmVH0, https://t.co/5Wf6qJ1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                User  \\\n",
       "Datetime                               \n",
       "2018-01-18 17:07:00  @GeroldSchlegel   \n",
       "2017-12-23 13:53:00  @derlamentierer   \n",
       "2017-12-05 19:47:00   @FlorianSchwab   \n",
       "2018-01-17 07:08:00   @Schweizer2018   \n",
       "2018-02-19 09:52:00   @PilgramsWords   \n",
       "\n",
       "                                                                 Tweet  \\\n",
       "Datetime                                                                 \n",
       "2018-01-18 17:07:00  RT @redder66: @NatalieRickli @NoBillag Die Unt...   \n",
       "2017-12-23 13:53:00  @ProBillag auch sie haben das konzept von soli...   \n",
       "2017-12-05 19:47:00  RT @VitalSteiner: Was die Gegner von #NoBillag...   \n",
       "2018-01-17 07:08:00  RT @realDanielVozar: Wenn #NoBillag abgelehnt ...   \n",
       "2018-02-19 09:52:00  RT @_macmike: #StarkesStück https://t.co/2L7uG...   \n",
       "\n",
       "                     Retweet           RT_from  Orig_Tweet  \\\n",
       "Datetime                                                     \n",
       "2018-01-18 17:07:00     True         @redder66       False   \n",
       "2017-12-23 13:53:00    False               NaN       False   \n",
       "2017-12-05 19:47:00     True     @VitalSteiner       False   \n",
       "2018-01-17 07:08:00     True  @realDanielVozar       False   \n",
       "2018-02-19 09:52:00     True         @_macmike       False   \n",
       "\n",
       "                                      Tweet_Participants  Reply  \\\n",
       "Datetime                                                          \n",
       "2018-01-18 17:07:00  [@NatalieRickli, @NoBillag, @SVPch]  False   \n",
       "2017-12-23 13:53:00                         [@ProBillag]   True   \n",
       "2017-12-05 19:47:00                                   []  False   \n",
       "2018-01-17 07:08:00                      [@LHeimgartner]  False   \n",
       "2018-02-19 09:52:00       [@Blickch, @20min, @bazonline]  False   \n",
       "\n",
       "                                                                  Text  \\\n",
       "Datetime                                                                 \n",
       "2018-01-18 17:07:00  Die Unterstützung der für die #NoBillag ergibt...   \n",
       "2017-12-23 13:53:00  auch sie haben das konzept von solidarität nic...   \n",
       "2017-12-05 19:47:00  Was die Gegner von #NoBillag mit solchen Bilde...   \n",
       "2018-01-17 07:08:00  Wenn #NoBillag abgelehnt wird, wird sich gar n...   \n",
       "2018-02-19 09:52:00  #StarkesStück  #AfD #SVP #ARD #ZDF #ORF #SRF #...   \n",
       "\n",
       "                                                                  Tags Lang  \\\n",
       "Datetime                                                                      \n",
       "2018-01-18 17:07:00                         [NoBillag, NeinzuNoBillag]   de   \n",
       "2017-12-23 13:53:00           [nobillag, jazunobillag, NeinzuNoBillag]   de   \n",
       "2017-12-05 19:47:00                                         [NoBillag]   de   \n",
       "2018-01-17 07:08:00                           [NoBillag, JazuNoBillag]   de   \n",
       "2018-02-19 09:52:00  [StarkesStück, AfD, SVP, ARD, ZDF, ORF, SRF, s...   de   \n",
       "\n",
       "                                                                  Link Label  \n",
       "Datetime                                                                      \n",
       "2018-01-18 17:07:00                                                 []     1  \n",
       "2017-12-23 13:53:00                          [https://t.co/iFLeHBuSFS]     6  \n",
       "2017-12-05 19:47:00                          [https://t.co/IKOSeftUlm]     6  \n",
       "2018-01-17 07:08:00                                                 []     5  \n",
       "2018-02-19 09:52:00  [https://t.co/2L7uGmmVH0, https://t.co/5Wf6qJ1...     0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kodierte Daten einlesen\n",
    "data_labeled = pd.read_pickle('data_modified/tweets_labeled.pkl')\n",
    "data_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sven\n",
      "[nltk_data]     Cornehls\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Funktionen definieren um Stoppworte zu entfernen & Daten weiter zu bereinigen\n",
    "nltk.download('stopwords')\n",
    "stopword_set = set(stopwords.words(\"german\"))\n",
    "def preprocess(raw_text):\n",
    "    res = raw_text\n",
    "    res = res.replace('ä', 'ae')\n",
    "    res = res.replace('ö', 'oe')\n",
    "    res = res.replace('ü', 'ue')\n",
    "    res = res.replace('Ä', 'Ae')\n",
    "    res = res.replace('Ö', 'Oe')\n",
    "    res = res.replace('Ü', 'Ue')\n",
    "    res = res.replace('ß', 'ss')\n",
    "    stopword_set = set(stopwords.words(\"german\"))\n",
    "    return \" \".join([i for i in re.sub(r'[^a-zA-Z\\s]', \"\", res).lower().split() if i not in stopword_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten bereinigen und X und y Variablen legen\n",
    "texts=[]\n",
    "for txt in data_labeled['Text']:\n",
    "    txt=preprocess(txt)\n",
    "    texts.append(txt)\n",
    "labels = data_labeled['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# CNN und Word2Vec-Parameter definieren\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "EMBEDDING_FILE = \"Path to: ...\\german.model\" #Muss wieder angepasst werden\n",
    "category_index = {\"Kann nicht zugordnet werden\": 0,\n",
    "                  \"Pressefreiheit\":1,\n",
    "                  \"Kommerzialisierung\":2,\n",
    "                  \"Mediale Grundversorgung\":3,\n",
    "                  \"Finanzen Haushalte\":4,\n",
    "                  \"SRG\":5,\n",
    "                  \"Libertarismus\":6,\n",
    "                  \"Pro\":7,\n",
    "                  \"Contra\":8}\n",
    "category_reverse_index = dict((y,x) for (x,y) in category_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5320 unique tokens.\n",
      "Shape of data tensor: (1034, 1000)\n",
      "Shape of label tensor: (1034, 9)\n",
      "Number of each frame in traing and validation set \n",
      "[167.  60.  40.  54.  18.  46.  54.  14.  64.]\n",
      "[177.  48.  41.  48.  21.  43.  44.  26.  69.]\n"
     ]
    }
   ],
   "source": [
    "# Daten (Texte) in richtige Form bringen und Trennen für Tests\n",
    "VALIDATION_SPLIT = 0.5\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "categories = to_categorical(np.asarray(labels))\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of labels:', categories.shape)\n",
    "\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "categories = categories[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = categories[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = categories[-nb_validation_samples:]\n",
    " \n",
    "print('Anzahl jedes Frames in validation set')\n",
    "print (y_train.sum(axis=0))\n",
    "print (y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 2836\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec-Layer vorbereiten\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
    "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-Modelle definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         1596300   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 499, 300)          270300    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 249, 150)          135150    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 124, 75)           33825     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9300)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9300)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               1395150   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 1359      \n",
      "=================================================================\n",
      "Total params: 3,432,084\n",
      "Trainable params: 1,835,784\n",
      "Non-trainable params: 1,596,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN-Modell 1 definieren\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))\n",
    "model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))\n",
    "model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(150,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(9,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 300)         1596300   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 998, 250)          225250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 2259      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 1,886,559\n",
      "Trainable params: 290,259\n",
      "Non-trainable params: 1,596,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN-Modell 2 definieren\n",
    "model_1 = Sequential()\n",
    "model_1.add(embedding_layer)\n",
    "model_1.add(Conv1D(250,3,padding='valid',activation='relu',strides=1))\n",
    "model_1.add(GlobalMaxPooling1D())\n",
    "model_1.add(Dense(250))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dense(9))\n",
    "model_1.add(Activation('sigmoid'))\n",
    "model_1.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - more complex convolutional neural network\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1000, 300)    1596300     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 998, 128)     115328      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 997, 128)     153728      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 996, 128)     192128      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 199, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 199, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 199, 128)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 597, 128)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 593, 128)     82048       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 118, 128)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 114, 128)     82048       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 3, 128)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 384)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          49280       flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 9)            1161        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,272,021\n",
      "Trainable params: 675,721\n",
      "Non-trainable params: 1,596,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN-Modell 3 definieren\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
    "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "# applying a more complex convolutional approach\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(filters=128,kernel_size=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(5)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_cov1= Conv1D(activation=\"relu\", filters=128, kernel_size=5)(l_merge)\n",
    "l_pool1 = MaxPooling1D(5)(l_cov1)\n",
    "l_cov2 = Conv1D(activation=\"relu\", filters=128, kernel_size=5)(l_pool1)\n",
    "l_pool2 = MaxPooling1D(30)(l_cov2)\n",
    "l_flat = Flatten()(l_pool2)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(9, activation='softmax')(l_dense)\n",
    "\n",
    "model_2 = Model(sequence_input, preds)\n",
    "model_2.compile(loss='categorical_crossentropy', \n",
    "                optimizer='rmsprop',\n",
    "                metrics=['acc'])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-Modelle ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 517 samples, validate on 517 samples\n",
      "Epoch 1/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 2.1679 - acc: 0.1760 - val_loss: 2.0532 - val_acc: 0.3424\n",
      "Epoch 2/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 2.0334 - acc: 0.3230 - val_loss: 2.0170 - val_acc: 0.3424\n",
      "Epoch 3/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 1.9824 - acc: 0.3230 - val_loss: 1.9921 - val_acc: 0.3424\n",
      "Epoch 4/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 1.9858 - acc: 0.3191 - val_loss: 2.0106 - val_acc: 0.3424\n",
      "Epoch 5/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 1.9397 - acc: 0.3230 - val_loss: 2.0110 - val_acc: 0.3424\n",
      "Epoch 6/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 1.9349 - acc: 0.3230 - val_loss: 1.9546 - val_acc: 0.3424\n",
      "Epoch 7/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 1.9011 - acc: 0.3230 - val_loss: 1.9454 - val_acc: 0.3424\n",
      "Epoch 8/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 1.8876 - acc: 0.3230 - val_loss: 1.9298 - val_acc: 0.3424\n",
      "Epoch 9/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 1.8498 - acc: 0.3230 - val_loss: 1.9817 - val_acc: 0.3424\n",
      "Epoch 10/10\n",
      "517/517 [==============================] - 8s 15ms/step - loss: 1.8321 - acc: 0.3269 - val_loss: 1.9520 - val_acc: 0.3443\n",
      "Test loss: 1.9519999737435199\n",
      "Test accuracy: 0.3442940038972943\n"
     ]
    }
   ],
   "source": [
    "# Modell 1 ausführen und Scores anschauen\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128)\n",
    "score = model.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 517 samples, validate on 517 samples\n",
      "Epoch 1/10\n",
      "517/517 [==============================] - 10s 19ms/step - loss: 2.0753 - acc: 0.2785 - val_loss: 2.0865 - val_acc: 0.3424\n",
      "Epoch 2/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.9233 - acc: 0.3230 - val_loss: 2.0417 - val_acc: 0.3424\n",
      "Epoch 3/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.8582 - acc: 0.3250 - val_loss: 2.0443 - val_acc: 0.3424\n",
      "Epoch 4/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.8194 - acc: 0.3230 - val_loss: 1.9406 - val_acc: 0.3424\n",
      "Epoch 5/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.7148 - acc: 0.3269 - val_loss: 2.0431 - val_acc: 0.3424\n",
      "Epoch 6/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.7098 - acc: 0.3327 - val_loss: 1.9560 - val_acc: 0.3424\n",
      "Epoch 7/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.6203 - acc: 0.3230 - val_loss: 1.9992 - val_acc: 0.3424\n",
      "Epoch 8/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.5875 - acc: 0.3385 - val_loss: 1.9589 - val_acc: 0.3424\n",
      "Epoch 9/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.4719 - acc: 0.4429 - val_loss: 2.0529 - val_acc: 0.3346\n",
      "Epoch 10/10\n",
      "517/517 [==============================] - 9s 18ms/step - loss: 1.3746 - acc: 0.5474 - val_loss: 2.0134 - val_acc: 0.2843\n",
      "Test loss: 2.013387449705393\n",
      "Test accuracy: 0.2843326886456525\n"
     ]
    }
   ],
   "source": [
    "# Modell 2 ausführen und Scores anschauen\n",
    "model_1.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128)\n",
    "score = model_1.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 517 samples, validate on 517 samples\n",
      "Epoch 1/10\n",
      "517/517 [==============================] - 20s 38ms/step - loss: 2.0696 - acc: 0.2437 - val_loss: 2.0355 - val_acc: 0.2921\n",
      "Epoch 2/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.9618 - acc: 0.3114 - val_loss: 1.9540 - val_acc: 0.2747\n",
      "Epoch 3/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.8810 - acc: 0.3191 - val_loss: 1.9590 - val_acc: 0.3443\n",
      "Epoch 4/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.8316 - acc: 0.3424 - val_loss: 2.0711 - val_acc: 0.3404\n",
      "Epoch 5/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.7864 - acc: 0.3501 - val_loss: 2.0089 - val_acc: 0.3424\n",
      "Epoch 6/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.7725 - acc: 0.3656 - val_loss: 2.0903 - val_acc: 0.2050\n",
      "Epoch 7/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.7779 - acc: 0.3520 - val_loss: 1.8962 - val_acc: 0.3172\n",
      "Epoch 8/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.6285 - acc: 0.4217 - val_loss: 1.9886 - val_acc: 0.3133\n",
      "Epoch 9/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.5966 - acc: 0.4352 - val_loss: 2.0001 - val_acc: 0.2824\n",
      "Epoch 10/10\n",
      "517/517 [==============================] - 19s 37ms/step - loss: 1.5661 - acc: 0.4739 - val_loss: 1.9678 - val_acc: 0.3075\n",
      "Test loss: 1.9678191834307732\n",
      "Test accuracy: 0.30754352030947774\n"
     ]
    }
   ],
   "source": [
    "# Modell 3 ausführen und Scores anschauen\n",
    "model_2.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10, batch_size=128)\n",
    "score = model_2.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfache Tests mit bestem Modell (wird nicht mehr ausgeführt):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweet = \"Bezahle lieber #Billag als Werbegetränkte Abo’s. Gefährlicher ist die wahrscheinliche Mediendominanz der reichen Sponsoren und Politakteure. NEIN zu #NoBillag ist ein Muss für Alle freiheitsliebenden Schweizer. Oder wollt ihr Spielball der Finanziers werden?\"\n",
    "example_tweet = preprocess(example_tweet)\n",
    "example_sequence = tokenizer.texts_to_sequences([example_tweet])\n",
    "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Predicted category: \", category_reverse_index[model_1.predict_classes(example_padded_sequence, verbose=0)[0]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "example_tweet = \"#NoBillag Aufklärung jenseits des linken Ja zu #NoBillag #Bolliger #Bio - Worüber SRF nicht informiert, Private aber schon: https://t.co/bHaGSgPaUf\"\n",
    "example_tweet = preprocess(example_tweet)\n",
    "example_sequence = tokenizer.texts_to_sequences([example_tweet])\n",
    "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Predicted category: \", category_reverse_index[model_1.predict_classes(example_padded_sequence, verbose=0)[0]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "probabilities = model_1.predict(example_padded_sequence, verbose=0)\n",
    "probabilities = probabilities[0]\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
